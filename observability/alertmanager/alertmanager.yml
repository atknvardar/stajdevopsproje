global:
  # Global SMTP configuration
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alertmanager@example.com'
  smtp_auth_username: 'alerts@company.com'
  smtp_auth_password: 'your-smtp-password'
  smtp_require_tls: true

  # Global Slack configuration
  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

# Email templates
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Alert routing configuration
route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  
  routes:
    # Chaos Engineering Alerts - Send to n8n for automated healing
    - match:
        alert_type: error_rate
      receiver: 'chaos-healing-webhook'
      group_wait: 5s
      repeat_interval: 30m
    
    - match:
        alert_type: latency  
      receiver: 'chaos-healing-webhook'
      group_wait: 5s
      repeat_interval: 30m
      
    - match:
        alert_type: memory
      receiver: 'chaos-healing-webhook'
      group_wait: 10s
      repeat_interval: 15m
      
    - match:
        alert_type: cpu
      receiver: 'chaos-healing-webhook'
      group_wait: 5s
      repeat_interval: 30m
      
    - match:
        alert_type: chaos_detected
      receiver: 'chaos-healing-webhook'
      group_wait: 0s
      repeat_interval: 5m

# Alert receivers and notification methods
receivers:
  # Default webhook receiver
  - name: 'web.hook'
    webhook_configs:
      - url: 'http://localhost:9093/api/v1/alerts'

  # n8n Chaos Healing Webhook
  - name: 'chaos-healing-webhook'
    webhook_configs:
      - url: 'http://n8n:5678/webhook/chaos-alert'
        send_resolved: true
        http_config:
          tls_config:
            insecure_skip_verify: true
        title: 'Chaos Engineering Alert - {{ .GroupLabels.alertname }}'
        text: |
          ðŸš¨ CHAOS ALERT DETECTED ðŸš¨
          
          Alert: {{ .GroupLabels.alertname }}
          Severity: {{ .GroupLabels.severity }}
          Service: {{ .GroupLabels.service }}
          Type: {{ .GroupLabels.alert_type }}
          
          {{ range .Alerts }}
          Summary: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Chaos Type: {{ .Annotations.chaos_type }}
          Time: {{ .StartsAt }}
          {{ end }}
          
          ðŸ”§ Automated healing workflow triggered via n8n
        
      # Backup webhook for logging
      - url: 'http://microservice:8080/admin/alert-received'
        send_resolved: true
        http_config:
          tls_config:
            insecure_skip_verify: true

# Inhibition rules - suppress redundant alerts
inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'dev', 'instance']

  # If a service is down, don't alert on high error rate
  - source_match:
      alertname: 'MicroserviceDown'
    target_match:
      alertname: 'HighHTTPErrorRate'
    equal: ['instance']

  # If all instances are down, don't alert on individual instances
  - source_match:
      alertname: 'MicroserviceInstancesDown'
    target_match:
      alertname: 'MicroserviceDown'
    equal: ['namespace']

  # If there's critical memory usage, don't alert on high memory usage
  - source_match:
      alertname: 'CriticalMemoryUsage'
    target_match:
      alertname: 'HighMemoryUsage'
    equal: ['pod']

  # If there's a critical error rate, don't alert on high error rate
  - source_match:
      alertname: 'CriticalHTTPErrorRate'
    target_match:
      alertname: 'HighHTTPErrorRate'
    equal: ['instance']

  # If there's very high response time, don't alert on high response time
  - source_match:
      alertname: 'VeryHighResponseTime'
    target_match:
      alertname: 'HighResponseTime'
    equal: ['instance']

  # If node is not ready, suppress other node alerts
  - source_match:
      alertname: 'NodeNotReady'
    target_match_re:
      alertname: 'Node.*'
    equal: ['node'] 